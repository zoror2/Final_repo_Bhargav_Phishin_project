{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d07c74f",
   "metadata": {},
   "source": [
    "## üìö Step 1: Import Libraries and Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29c8a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, accuracy_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Keras version: {keras.__version__}\")\n",
    "\n",
    "# Check GPU availability\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üîç GPU Detection\")\n",
    "print(\"=\"*70)\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"‚úÖ Found {len(gpus)} GPU(s):\")\n",
    "    for i, gpu in enumerate(gpus):\n",
    "        print(f\"   GPU {i}: {gpu.name}\")\n",
    "    \n",
    "    # Enable memory growth to prevent TensorFlow from allocating all GPU memory at once\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    print(\"\\n‚úÖ Memory growth enabled for all GPUs\")\n",
    "else:\n",
    "    print(\"‚ùå No GPU found! Training will be slow on CPU.\")\n",
    "    print(\"   Make sure to enable GPU accelerator in Kaggle (Settings > Accelerator > GPU T4 x2)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3816f37",
   "metadata": {},
   "source": [
    "## üéØ Step 2: Configure Multi-GPU Training Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a48e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure TensorFlow MirroredStrategy for multi-GPU training\n",
    "print(\"=\"*70)\n",
    "print(\"‚ö° Configuring Multi-GPU Training Strategy\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create a MirroredStrategy for synchronous training across all GPUs\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "print(f\"\\n‚úÖ MirroredStrategy initialized\")\n",
    "print(f\"   Number of devices: {strategy.num_replicas_in_sync}\")\n",
    "print(f\"   Device names: {strategy.extended.worker_devices}\")\n",
    "\n",
    "if strategy.num_replicas_in_sync >= 2:\n",
    "    print(f\"\\nüî• Multi-GPU training enabled with {strategy.num_replicas_in_sync} GPUs!\")\n",
    "    print(f\"   Effective batch size will be: BATCH_SIZE √ó {strategy.num_replicas_in_sync}\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è Only {strategy.num_replicas_in_sync} GPU detected. Enable 2x T4 for faster training.\")\n",
    "\n",
    "# Global batch size (will be distributed across GPUs)\n",
    "BATCH_SIZE_PER_REPLICA = 64\n",
    "GLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\n",
    "\n",
    "print(f\"\\nüìä Batch size configuration:\")\n",
    "print(f\"   Per GPU: {BATCH_SIZE_PER_REPLICA}\")\n",
    "print(f\"   Global (effective): {GLOBAL_BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6df805",
   "metadata": {},
   "source": [
    "## üìÇ Step 3: Load and Explore Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eec4b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "print(\"=\"*70)\n",
    "print(\"üìÇ Loading Dataset\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Try multiple paths (Kaggle input path and local path)\n",
    "dataset_paths = [\n",
    "    '/kaggle/input/final-lstm-traning-dataset/final_lstm_training_dataset.csv',  # Kaggle path\n",
    "    'final_lstm_training_dataset.csv',                                             # Local path\n",
    "    r'd:\\Phishing LSTM Model\\final_lstm_training_dataset.csv'                     # Absolute local path\n",
    "]\n",
    "\n",
    "df = None\n",
    "for path in dataset_paths:\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "        print(f\"‚úÖ Dataset loaded successfully from: {path}\")\n",
    "        break\n",
    "    except (FileNotFoundError, Exception) as e:\n",
    "        print(f\"   ‚è≠Ô∏è Skipping {path}: {type(e).__name__}\")\n",
    "        continue\n",
    "\n",
    "if df is None:\n",
    "    raise FileNotFoundError(\"‚ùå Could not find dataset! Please upload 'final_lstm_training_dataset.csv' to Kaggle.\")\n",
    "\n",
    "# Display basic information\n",
    "print(f\"\\nüìä Dataset Shape: {df.shape}\")\n",
    "print(f\"   Rows: {df.shape[0]:,}\")\n",
    "print(f\"   Columns: {df.shape[1]}\")\n",
    "\n",
    "# IMPORTANT: Check if data was loaded correctly\n",
    "print(f\"\\nüîç Data Loading Check:\")\n",
    "print(f\"   Total rows loaded: {len(df):,}\")\n",
    "print(f\"   Expected: ~24,680 rows\")\n",
    "if len(df) < 20000:\n",
    "    print(f\"   ‚ö†Ô∏è WARNING: Only {len(df):,} rows loaded! Expected ~24,680 rows.\")\n",
    "    print(f\"   This may indicate a problem with the dataset file.\")\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = df.isnull().sum().sum()\n",
    "print(f\"\\nüîç Missing values: {missing_values}\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nüìã First 3 rows:\")\n",
    "display(df.head(3))\n",
    "\n",
    "# Display column names\n",
    "print(f\"\\nüìù Column names ({len(df.columns)} total):\")\n",
    "for i, col in enumerate(df.columns, 1):\n",
    "    print(f\"   {i:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c1b00e",
   "metadata": {},
   "source": [
    "## üîç Step 4: Data Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ece12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze label distribution\n",
    "print(\"=\"*70)\n",
    "print(\"üìä Label Distribution Analysis\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "label_counts = df['label'].value_counts().sort_index()\n",
    "print(\"\\n‚öñÔ∏è Class distribution:\")\n",
    "for label, count in label_counts.items():\n",
    "    label_name = \"Legitimate\" if label == 0 else \"Phishing\"\n",
    "    percentage = (count / len(df)) * 100\n",
    "    print(f\"   {label_name} ({label}): {count:,} samples ({percentage:.2f}%)\")\n",
    "\n",
    "# Calculate imbalance ratio\n",
    "imbalance_ratio = label_counts.max() / label_counts.min()\n",
    "print(f\"\\nüìà Imbalance ratio: {imbalance_ratio:.2f}:1\")\n",
    "\n",
    "# Visualize label distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar plot\n",
    "ax1 = axes[0]\n",
    "colors = ['#2ecc71', '#e74c3c']\n",
    "bars = ax1.bar(['Legitimate (0)', 'Phishing (1)'], label_counts.values, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax1.set_ylabel('Count', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Class Distribution', fontsize=14, fontweight='bold')\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add count labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{int(height):,}',\n",
    "             ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Pie chart\n",
    "ax2 = axes[1]\n",
    "ax2.pie(label_counts.values, labels=['Legitimate (0)', 'Phishing (1)'], \n",
    "        colors=colors, autopct='%1.1f%%', startangle=90, \n",
    "        textprops={'fontsize': 11, 'fontweight': 'bold'})\n",
    "ax2.set_title('Class Proportion', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Check for duplicates\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"\\nüîÑ Duplicate rows: {duplicates}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699b6460",
   "metadata": {},
   "source": [
    "## üßπ Step 5: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50b1fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"üßπ Data Preprocessing\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Make a copy\n",
    "df_clean = df.copy()\n",
    "print(f\"Starting with: {len(df_clean):,} rows\")\n",
    "\n",
    "# Remove URL column (not needed for training)\n",
    "if 'url' in df_clean.columns:\n",
    "    df_clean = df_clean.drop('url', axis=1)\n",
    "    print(f\"‚úÖ Removed 'url' column, rows: {len(df_clean):,}\")\n",
    "\n",
    "# Convert ALL boolean string columns to numeric automatically\n",
    "print(f\"\\nüîÑ Converting data types...\")\n",
    "\n",
    "# Check all columns for string boolean values\n",
    "for col in df_clean.columns:\n",
    "    if col == 'label':  # Skip the target column\n",
    "        continue\n",
    "    \n",
    "    # Check if column contains string boolean values\n",
    "    if df_clean[col].dtype == 'object':\n",
    "        unique_values = set(df_clean[col].dropna().unique())\n",
    "        # Check if values are boolean strings\n",
    "        if unique_values.issubset({'True', 'False', 'true', 'false', '1', '0'}):\n",
    "            # Convert to numeric\n",
    "            df_clean[col] = df_clean[col].map({\n",
    "                'True': 1, 'False': 0, \n",
    "                'true': 1, 'false': 0,\n",
    "                True: 1, False: 0,\n",
    "                '1': 1, '0': 0,\n",
    "                1: 1, 0: 0\n",
    "            })\n",
    "            print(f\"   Converted '{col}' from string to numeric\")\n",
    "        else:\n",
    "            # Try to convert to numeric if possible\n",
    "            try:\n",
    "                df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n",
    "                print(f\"   Converted '{col}' to numeric (with coercion)\")\n",
    "            except:\n",
    "                print(f\"   ‚ö†Ô∏è Warning: Column '{col}' contains non-numeric values\")\n",
    "\n",
    "# Handle missing values\n",
    "print(f\"\\nüîç Handling missing values...\")\n",
    "before_missing = df_clean.isnull().sum().sum()\n",
    "print(f\"   Before: {before_missing} missing values\")\n",
    "\n",
    "# Check which columns have missing values\n",
    "if before_missing > 0:\n",
    "    print(f\"\\n   Columns with missing values:\")\n",
    "    missing_cols = df_clean.isnull().sum()\n",
    "    for col, count in missing_cols[missing_cols > 0].items():\n",
    "        print(f\"      {col}: {count} missing\")\n",
    "\n",
    "df_clean = df_clean.fillna(0)\n",
    "after_missing = df_clean.isnull().sum().sum()\n",
    "print(f\"   After: {after_missing} missing values\")\n",
    "print(f\"   Rows after filling: {len(df_clean):,}\")\n",
    "\n",
    "# Remove duplicates - DISABLED to keep all training samples\n",
    "# Many URLs have similar features, so we want to keep them all for training\n",
    "print(f\"\\nüóëÔ∏è Checking for duplicates...\")\n",
    "before_rows = len(df_clean)\n",
    "duplicates_count = df_clean.duplicated().sum()\n",
    "print(f\"   Found {duplicates_count} rows with duplicate feature values\")\n",
    "print(f\"   ‚ö†Ô∏è KEEPING all rows (including duplicates) for training\")\n",
    "print(f\"   Reason: Different URLs can have similar features - we want to learn from all samples\")\n",
    "\n",
    "# Uncomment the line below if you want to remove duplicates\n",
    "# df_clean = df_clean.drop_duplicates()\n",
    "\n",
    "after_rows = len(df_clean)\n",
    "print(f\"   Final row count: {after_rows:,} rows retained\")\n",
    "\n",
    "# Separate features and labels\n",
    "X = df_clean.drop('label', axis=1)\n",
    "y = df_clean['label']\n",
    "\n",
    "print(f\"\\n‚úÖ Preprocessing complete!\")\n",
    "print(f\"   Features shape: {X.shape}\")\n",
    "print(f\"   Labels shape: {y.shape}\")\n",
    "print(f\"   Number of features: {X.shape[1]}\")\n",
    "print(f\"   Final samples for training: {len(X):,}\")\n",
    "\n",
    "# Verify all columns are numeric\n",
    "non_numeric = X.select_dtypes(include=['object']).columns.tolist()\n",
    "if non_numeric:\n",
    "    print(f\"\\n‚ö†Ô∏è WARNING: Non-numeric columns detected: {non_numeric}\")\n",
    "    print(f\"   Attempting to display unique values:\")\n",
    "    for col in non_numeric:\n",
    "        unique_vals = X[col].unique()[:10]  # Show first 10 unique values\n",
    "        print(f\"      {col}: {unique_vals}\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ All feature columns are numeric\")\n",
    "\n",
    "# Display feature names\n",
    "print(f\"\\nüìù Feature columns ({len(X.columns)} total):\")\n",
    "for i, col in enumerate(X.columns, 1):\n",
    "    dtype = X[col].dtype\n",
    "    print(f\"   {i:2d}. {col:30s} (dtype: {dtype})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c84d2a4",
   "metadata": {},
   "source": [
    "## ‚úÇÔ∏è Step 6: Split Data into Train/Validation/Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f929eb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"‚úÇÔ∏è Splitting Data\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Split: 70% train, 15% validation, 15% test\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.15, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.176, random_state=42, stratify=y_temp  # 0.176 of 85% ‚âà 15% of total\n",
    ")\n",
    "\n",
    "print(\"\\nüìä Dataset split sizes:\")\n",
    "print(f\"   Training set:   {len(X_train):,} samples ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"   Validation set: {len(X_val):,} samples ({len(X_val)/len(X)*100:.1f}%)\")\n",
    "print(f\"   Test set:       {len(X_test):,} samples ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "\n",
    "# Check class distribution in each set\n",
    "print(\"\\n‚öñÔ∏è Class distribution in each set:\")\n",
    "for name, labels in [('Train', y_train), ('Validation', y_val), ('Test', y_test)]:\n",
    "    counts = labels.value_counts().sort_index()\n",
    "    print(f\"\\n   {name}:\")\n",
    "    for label, count in counts.items():\n",
    "        label_name = \"Legitimate\" if label == 0 else \"Phishing\"\n",
    "        print(f\"      {label_name} ({label}): {count:,} ({count/len(labels)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a55c956",
   "metadata": {},
   "source": [
    "## üîß Step 7: Feature Scaling and Sequence Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee040c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"üîß Feature Scaling and LSTM Preparation\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Standardize features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"‚úÖ Features scaled using StandardScaler\")\n",
    "\n",
    "# Reshape for LSTM: (samples, timesteps, features)\n",
    "# For this dataset, we treat each feature vector as a single timestep\n",
    "X_train_lstm = X_train_scaled.reshape(X_train_scaled.shape[0], 1, X_train_scaled.shape[1])\n",
    "X_val_lstm = X_val_scaled.reshape(X_val_scaled.shape[0], 1, X_val_scaled.shape[1])\n",
    "X_test_lstm = X_test_scaled.reshape(X_test_scaled.shape[0], 1, X_test_scaled.shape[1])\n",
    "\n",
    "print(f\"\\nüìê LSTM input shapes:\")\n",
    "print(f\"   Training:   {X_train_lstm.shape} (samples, timesteps, features)\")\n",
    "print(f\"   Validation: {X_val_lstm.shape}\")\n",
    "print(f\"   Test:       {X_test_lstm.shape}\")\n",
    "\n",
    "# Convert labels to numpy arrays\n",
    "y_train_arr = y_train.values\n",
    "y_val_arr = y_val.values\n",
    "y_test_arr = y_test.values\n",
    "\n",
    "print(f\"\\n‚úÖ Data preparation complete!\")\n",
    "print(f\"   Ready for LSTM training with {X_train_lstm.shape[2]} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd45074",
   "metadata": {},
   "source": [
    "## ‚öñÔ∏è Step 8: Calculate Class Weights for Imbalanced Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7095d709",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"‚öñÔ∏è Computing Class Weights\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Calculate class weights to handle imbalanced dataset\n",
    "class_weights_array = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train_arr),\n",
    "    y=y_train_arr\n",
    ")\n",
    "\n",
    "class_weights = {i: weight for i, weight in enumerate(class_weights_array)}\n",
    "\n",
    "print(\"\\nüìä Class weights (to balance training):\")\n",
    "for class_idx, weight in class_weights.items():\n",
    "    class_name = \"Legitimate\" if class_idx == 0 else \"Phishing\"\n",
    "    print(f\"   {class_name} ({class_idx}): {weight:.4f}\")\n",
    "\n",
    "print(f\"\\nüí° Higher weight ({max(class_weights.values()):.4f}) will be applied to minority class\")\n",
    "print(\"   This helps the model learn from underrepresented samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eac80c7",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Step 9: Build LSTM Model Architecture (Multi-GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180de524",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"üèóÔ∏è Building LSTM Model Architecture\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Build model inside strategy scope for multi-GPU training\n",
    "with strategy.scope():\n",
    "    \n",
    "    # Define the model\n",
    "    model = Sequential([\n",
    "        # First Bidirectional LSTM layer\n",
    "        Bidirectional(LSTM(128, return_sequences=True), \n",
    "                     input_shape=(X_train_lstm.shape[1], X_train_lstm.shape[2])),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        # Second Bidirectional LSTM layer\n",
    "        Bidirectional(LSTM(64, return_sequences=False)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        # Dense layers\n",
    "        Dense(64, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        # Output layer\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), tf.keras.metrics.AUC()]\n",
    "    )\n",
    "\n",
    "print(\"\\n‚úÖ Model built successfully inside MirroredStrategy scope!\")\n",
    "print(f\"   Model will train across {strategy.num_replicas_in_sync} GPU(s)\\n\")\n",
    "\n",
    "# Display model architecture\n",
    "model.summary()\n",
    "\n",
    "# Count parameters\n",
    "trainable_params = sum([np.prod(v.shape) for v in model.trainable_variables])\n",
    "print(f\"\\nüìä Total trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fb251c",
   "metadata": {},
   "source": [
    "## üéØ Step 10: Configure Training Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0de26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"üéØ Configuring Training Callbacks\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create callbacks for training optimization\n",
    "callbacks = [\n",
    "    # Early stopping: stop training if validation loss doesn't improve\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=15,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # Reduce learning rate when validation loss plateaus\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=0.00001,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # Save best model checkpoint\n",
    "    ModelCheckpoint(\n",
    "        'best_phishing_lstm_model.h5',\n",
    "        monitor='val_accuracy',\n",
    "        mode='max',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # TensorBoard logging\n",
    "    TensorBoard(\n",
    "        log_dir='./logs',\n",
    "        histogram_freq=1,\n",
    "        write_graph=True\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"\\n‚úÖ Configured callbacks:\")\n",
    "print(\"   1. Early Stopping (patience=15)\")\n",
    "print(\"   2. Reduce Learning Rate on Plateau (patience=5, factor=0.5)\")\n",
    "print(\"   3. Model Checkpoint (save best model)\")\n",
    "print(\"   4. TensorBoard Logging\")\n",
    "\n",
    "# Training configuration\n",
    "EPOCHS = 100\n",
    "print(f\"\\n‚öôÔ∏è Training configuration:\")\n",
    "print(f\"   Max epochs: {EPOCHS}\")\n",
    "print(f\"   Batch size per GPU: {BATCH_SIZE_PER_REPLICA}\")\n",
    "print(f\"   Global batch size: {GLOBAL_BATCH_SIZE}\")\n",
    "print(f\"   Learning rate: 0.001 (will reduce on plateau)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6a394e",
   "metadata": {},
   "source": [
    "## üî• Step 11: Train the Model (Multi-GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cc401c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"üî• Starting Multi-GPU Training\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n‚ö° Training across {strategy.num_replicas_in_sync} GPU(s)...\\n\")\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train_lstm, y_train_arr,\n",
    "    validation_data=(X_val_lstm, y_val_arr),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=GLOBAL_BATCH_SIZE,  # This batch size is distributed across GPUs\n",
    "    class_weight=class_weights,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ Training Complete!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n‚è±Ô∏è Total training time: {training_time/60:.2f} minutes ({training_time:.0f} seconds)\")\n",
    "print(f\"   Epochs completed: {len(history.history['loss'])}\")\n",
    "print(f\"   Final training accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
    "print(f\"   Final validation accuracy: {history.history['val_accuracy'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcf954e",
   "metadata": {},
   "source": [
    "## üìä Step 12: Visualize Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9271deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"üìä Visualizing Training History\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create comprehensive training visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "fig.suptitle('LSTM Model Training History - Multi-GPU', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Loss\n",
    "axes[0, 0].plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
    "axes[0, 0].plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "axes[0, 0].set_xlabel('Epoch', fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Loss', fontweight='bold')\n",
    "axes[0, 0].set_title('Model Loss', fontweight='bold')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Accuracy\n",
    "axes[0, 1].plot(history.history['accuracy'], label='Training Accuracy', linewidth=2)\n",
    "axes[0, 1].plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
    "axes[0, 1].set_xlabel('Epoch', fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Accuracy', fontweight='bold')\n",
    "axes[0, 1].set_title('Model Accuracy', fontweight='bold')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Precision\n",
    "axes[0, 2].plot(history.history['precision'], label='Training Precision', linewidth=2)\n",
    "axes[0, 2].plot(history.history['val_precision'], label='Validation Precision', linewidth=2)\n",
    "axes[0, 2].set_xlabel('Epoch', fontweight='bold')\n",
    "axes[0, 2].set_ylabel('Precision', fontweight='bold')\n",
    "axes[0, 2].set_title('Model Precision', fontweight='bold')\n",
    "axes[0, 2].legend()\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Recall\n",
    "axes[1, 0].plot(history.history['recall'], label='Training Recall', linewidth=2)\n",
    "axes[1, 0].plot(history.history['val_recall'], label='Validation Recall', linewidth=2)\n",
    "axes[1, 0].set_xlabel('Epoch', fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Recall', fontweight='bold')\n",
    "axes[1, 0].set_title('Model Recall', fontweight='bold')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 5. AUC\n",
    "axes[1, 1].plot(history.history['auc'], label='Training AUC', linewidth=2)\n",
    "axes[1, 1].plot(history.history['val_auc'], label='Validation AUC', linewidth=2)\n",
    "axes[1, 1].set_xlabel('Epoch', fontweight='bold')\n",
    "axes[1, 1].set_ylabel('AUC', fontweight='bold')\n",
    "axes[1, 1].set_title('Model AUC', fontweight='bold')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Learning Rate (if available)\n",
    "if 'lr' in history.history:\n",
    "    axes[1, 2].plot(history.history['lr'], linewidth=2, color='orange')\n",
    "    axes[1, 2].set_xlabel('Epoch', fontweight='bold')\n",
    "    axes[1, 2].set_ylabel('Learning Rate', fontweight='bold')\n",
    "    axes[1, 2].set_title('Learning Rate Schedule', fontweight='bold')\n",
    "    axes[1, 2].set_yscale('log')\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "else:\n",
    "    axes[1, 2].text(0.5, 0.5, 'Learning Rate\\nNot Tracked', \n",
    "                   ha='center', va='center', fontsize=12)\n",
    "    axes[1, 2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Training visualization saved as 'training_history.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bc875f",
   "metadata": {},
   "source": [
    "## üéØ Step 13: Evaluate Model on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d54f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"üéØ Evaluating Model on Test Set\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_results = model.evaluate(X_test_lstm, y_test_arr, verbose=1)\n",
    "\n",
    "print(\"\\nüìä Test Set Performance:\")\n",
    "metric_names = ['Loss', 'Accuracy', 'Precision', 'Recall', 'AUC']\n",
    "for name, value in zip(metric_names, test_results):\n",
    "    print(f\"   {name:12s}: {value:.4f}\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_prob = model.predict(X_test_lstm)\n",
    "y_pred = (y_pred_prob > 0.5).astype(int).flatten()\n",
    "\n",
    "# Calculate additional metrics\n",
    "from sklearn.metrics import f1_score, matthews_corrcoef\n",
    "\n",
    "f1 = f1_score(y_test_arr, y_pred)\n",
    "mcc = matthews_corrcoef(y_test_arr, y_pred)\n",
    "\n",
    "print(f\"\\nüìà Additional Metrics:\")\n",
    "print(f\"   F1-Score: {f1:.4f}\")\n",
    "print(f\"   Matthews Correlation Coefficient: {mcc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14404431",
   "metadata": {},
   "source": [
    "## üìã Step 14: Classification Report and Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee5404a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"üìã Detailed Classification Report\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Classification report\n",
    "class_names = ['Legitimate (0)', 'Phishing (1)']\n",
    "print(\"\\n\" + classification_report(y_test_arr, y_pred, target_names=class_names, digits=4))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test_arr, y_pred)\n",
    "\n",
    "# Visualize confusion matrix\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Raw counts\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, \n",
    "            yticklabels=class_names, ax=axes[0], cbar_kws={'label': 'Count'})\n",
    "axes[0].set_xlabel('Predicted Label', fontweight='bold', fontsize=12)\n",
    "axes[0].set_ylabel('True Label', fontweight='bold', fontsize=12)\n",
    "axes[0].set_title('Confusion Matrix (Counts)', fontweight='bold', fontsize=14)\n",
    "\n",
    "# Normalized\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Greens', xticklabels=class_names, \n",
    "            yticklabels=class_names, ax=axes[1], cbar_kws={'label': 'Percentage'})\n",
    "axes[1].set_xlabel('Predicted Label', fontweight='bold', fontsize=12)\n",
    "axes[1].set_ylabel('True Label', fontweight='bold', fontsize=12)\n",
    "axes[1].set_title('Confusion Matrix (Normalized)', fontweight='bold', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Confusion matrix saved as 'confusion_matrix.png'\")\n",
    "\n",
    "# Print confusion matrix interpretation\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "print(\"\\nüîç Confusion Matrix Breakdown:\")\n",
    "print(f\"   True Negatives (Legitimate correctly classified):  {tn:,}\")\n",
    "print(f\"   False Positives (Legitimate wrongly as Phishing):  {fp:,}\")\n",
    "print(f\"   False Negatives (Phishing wrongly as Legitimate):  {fn:,}\")\n",
    "print(f\"   True Positives (Phishing correctly classified):    {tp:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a8da65",
   "metadata": {},
   "source": [
    "## üìà Step 15: ROC Curve and AUC Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00054641",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"üìà ROC Curve Analysis\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Calculate ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test_arr, y_pred_prob)\n",
    "roc_auc = roc_auc_score(y_test_arr, y_pred_prob)\n",
    "\n",
    "print(f\"\\nüéØ ROC AUC Score: {roc_auc:.4f}\")\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier (AUC = 0.5)')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontweight='bold', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontweight='bold', fontsize=12)\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve', fontweight='bold', fontsize=14)\n",
    "plt.legend(loc='lower right', fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig('roc_curve.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ ROC curve saved as 'roc_curve.png'\")\n",
    "\n",
    "# Find optimal threshold\n",
    "optimal_idx = np.argmax(tpr - fpr)\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "print(f\"\\nüéØ Optimal threshold: {optimal_threshold:.4f}\")\n",
    "print(f\"   At this threshold:\")\n",
    "print(f\"   - True Positive Rate: {tpr[optimal_idx]:.4f}\")\n",
    "print(f\"   - False Positive Rate: {fpr[optimal_idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90718f0f",
   "metadata": {},
   "source": [
    "## üíæ Step 16: Save Model and Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604ac3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"üíæ Saving Model and Preprocessing Components\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Save the final model\n",
    "model.save('phishing_lstm_model_final.h5')\n",
    "print(\"\\n‚úÖ Model saved as 'phishing_lstm_model_final.h5'\")\n",
    "\n",
    "# Save the scaler\n",
    "import joblib\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "print(\"‚úÖ Scaler saved as 'scaler.pkl'\")\n",
    "\n",
    "# Save feature names\n",
    "feature_names = X.columns.tolist()\n",
    "with open('feature_names.txt', 'w') as f:\n",
    "    f.write('\\n'.join(feature_names))\n",
    "print(\"‚úÖ Feature names saved as 'feature_names.txt'\")\n",
    "\n",
    "# Save training history\n",
    "import json\n",
    "history_dict = {key: [float(val) for val in values] for key, values in history.history.items()}\n",
    "with open('training_history.json', 'w') as f:\n",
    "    json.dump(history_dict, f, indent=2)\n",
    "print(\"‚úÖ Training history saved as 'training_history.json'\")\n",
    "\n",
    "print(\"\\nüì¶ All files ready for download:\")\n",
    "print(\"   1. phishing_lstm_model_final.h5 - Trained LSTM model\")\n",
    "print(\"   2. best_phishing_lstm_model.h5 - Best checkpoint during training\")\n",
    "print(\"   3. scaler.pkl - Feature scaler for preprocessing\")\n",
    "print(\"   4. feature_names.txt - List of feature names\")\n",
    "print(\"   5. training_history.json - Training metrics history\")\n",
    "print(\"   6. training_history.png - Training visualization\")\n",
    "print(\"   7. confusion_matrix.png - Confusion matrix visualization\")\n",
    "print(\"   8. roc_curve.png - ROC curve visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa2442e",
   "metadata": {},
   "source": [
    "## üìä Step 17: Final Performance Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9001ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"üéâ TRAINING COMPLETE - FINAL SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n‚öôÔ∏è Training Configuration:\")\n",
    "print(f\"   GPUs used: {strategy.num_replicas_in_sync}\")\n",
    "print(f\"   Total training time: {training_time/60:.2f} minutes\")\n",
    "print(f\"   Epochs completed: {len(history.history['loss'])}\")\n",
    "print(f\"   Global batch size: {GLOBAL_BATCH_SIZE}\")\n",
    "\n",
    "print(\"\\nüìä Dataset Information:\")\n",
    "print(f\"   Total samples: {len(df_clean):,}\")\n",
    "print(f\"   Features: {X_train_lstm.shape[2]}\")\n",
    "print(f\"   Training samples: {len(X_train):,}\")\n",
    "print(f\"   Validation samples: {len(X_val):,}\")\n",
    "print(f\"   Test samples: {len(X_test):,}\")\n",
    "\n",
    "print(\"\\nüéØ Final Test Performance:\")\n",
    "print(f\"   Accuracy:  {test_results[1]:.4f} ({test_results[1]*100:.2f}%)\")\n",
    "print(f\"   Precision: {test_results[2]:.4f}\")\n",
    "print(f\"   Recall:    {test_results[3]:.4f}\")\n",
    "print(f\"   AUC:       {test_results[4]:.4f}\")\n",
    "print(f\"   F1-Score:  {f1:.4f}\")\n",
    "\n",
    "print(\"\\nüîç Classification Breakdown:\")\n",
    "print(f\"   True Negatives:  {tn:,}\")\n",
    "print(f\"   False Positives: {fp:,}\")\n",
    "print(f\"   False Negatives: {fn:,}\")\n",
    "print(f\"   True Positives:  {tp:,}\")\n",
    "\n",
    "print(\"\\n‚úÖ Model files saved and ready for deployment!\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Thank you for using this notebook! üöÄ\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b837c75",
   "metadata": {},
   "source": [
    "## üß™ Step 18: Test Model with Sample Predictions (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acd8ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Test the model with a few sample predictions\n",
    "print(\"=\"*70)\n",
    "print(\"üß™ Sample Predictions\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get 10 random samples from test set\n",
    "np.random.seed(42)\n",
    "sample_indices = np.random.choice(len(X_test_lstm), size=10, replace=False)\n",
    "\n",
    "print(\"\\nüìã Sample predictions from test set:\\n\")\n",
    "for idx in sample_indices:\n",
    "    true_label = y_test_arr[idx]\n",
    "    pred_prob = y_pred_prob[idx][0]\n",
    "    pred_label = 1 if pred_prob > 0.5 else 0\n",
    "    \n",
    "    true_name = \"Phishing\" if true_label == 1 else \"Legitimate\"\n",
    "    pred_name = \"Phishing\" if pred_label == 1 else \"Legitimate\"\n",
    "    correct = \"‚úÖ\" if true_label == pred_label else \"‚ùå\"\n",
    "    \n",
    "    print(f\"Sample {idx:4d}: True={true_name:10s} | Pred={pred_name:10s} ({pred_prob:.4f}) {correct}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
